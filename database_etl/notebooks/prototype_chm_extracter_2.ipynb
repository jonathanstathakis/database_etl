{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A redo of the chemstation data and metadata extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rainbow as rb\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "d_path = \"/Users/jonathan/mres_thesis/database_etl/tests/data/agilent_D/130.D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = rb.read(str(Path(d_path)))\n",
    "datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_data = datadir.get_file(filename=\"DAD1.UV\")\n",
    "xlabels = uv_data.xlabels.ravel()\n",
    "ylabels = uv_data.ylabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pd.DataFrame(index=xlabels, columns=ylabels, data=uv_data.data)\n",
    "    .rename_axis(\"time\")\n",
    "    .rename_axis(\"nm\", axis=1)\n",
    "    .reset_index()\n",
    "    .pipe(pl.from_pandas)\n",
    "    .with_columns(pl.col(\"time\").sub(pl.col(\"time\").first()))\n",
    "    .with_columns(\n",
    "        pl.lit(datadir.metadata[\"id\"]).alias(\"id\"), pl.exclude([\"id\", \"time\"])\n",
    "    )\n",
    "    .select(\"id\", \"time\", pl.exclude([\"id\", \"time\"]))\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so that's done. write it back to the original data file. Do the same for the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {**uv_data.metadata, **datadir.metadata}\n",
    "metadata = {\n",
    "    key.lower(): val\n",
    "    for key, val in metadata.items()\n",
    "    if key not in [\"unit\", \"vendor\", \"signal\"]\n",
    "}\n",
    "pl.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also write that back to the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok so now they are good to go, copy over to this projects data file all the raw uv files then run as planned - write the metadata csv and data csvs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to track progress, we'll write the extraction to a dir 'extraction', named 'extraction_<datetime>'. Thus on a rerun if an \"extraction_\" file is detected, the date time is parsed and an error is thrown. user can choose to overwrite if they want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "\n",
    "def get_data(path: Path) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    datadir = rb.read(str(Path(path)))\n",
    "\n",
    "    # image\n",
    "    uv_data = datadir.get_file(filename=\"DAD1.UV\")\n",
    "    xlabels = uv_data.xlabels.ravel()\n",
    "    ylabels = uv_data.ylabels\n",
    "    data = (\n",
    "        pd.DataFrame(index=xlabels, columns=ylabels, data=uv_data.data)\n",
    "        .rename_axis(\"time\")\n",
    "        .rename_axis(\"nm\", axis=1)\n",
    "        .reset_index()\n",
    "        .pipe(pl.from_pandas)\n",
    "        .with_columns(pl.col(\"time\").sub(pl.col(\"time\").first()))\n",
    "        .with_columns(\n",
    "            pl.lit(datadir.metadata[\"id\"]).alias(\"id\"), pl.exclude([\"id\", \"time\"])\n",
    "        )\n",
    "        .select(\"id\", \"time\", pl.exclude([\"id\", \"time\"]))\n",
    "    )\n",
    "\n",
    "    # run metadata\n",
    "    metadata = {**uv_data.metadata, **datadir.metadata}\n",
    "\n",
    "    metadata = {\n",
    "        key.lower().replace(\" \", \"_\"): val\n",
    "        for key, val in metadata.items()\n",
    "        if key not in [\"unit\", \"vendor\", \"signal\"]\n",
    "    }\n",
    "    return pl.DataFrame(metadata), data\n",
    "\n",
    "\n",
    "def write_extraction_dir(extract_path_prefix, data, metadata) -> str:\n",
    "    time_now = datetime.now().isoformat(timespec=\"seconds\").replace(\":\", \"\")\n",
    "    extract_dir = extract_path_prefix.parent / (\n",
    "        str(extract_path_prefix.name) + f\"{time_now}\"\n",
    "    )\n",
    "    extract_dir.mkdir()\n",
    "    metadata_out = extract_dir / \"metadata.parquet\"\n",
    "    data_out = extract_dir / \"data.parquet\"\n",
    "\n",
    "    metadata.write_parquet(metadata_out)\n",
    "    data.write_parquet(data_out)\n",
    "\n",
    "    return f\"wrote extracted data to {extract_dir}\"\n",
    "\n",
    "\n",
    "x = 0\n",
    "\n",
    "\n",
    "def extract_run_data(path: Path, overwrite=True):\n",
    "    \"\"\"\n",
    "    Extract the metadata and data of each .D in `path` as parquets dirwise stored within the .D file under \"extract_<current datetime>\".\n",
    "\n",
    "    :overwrite: if True, will overwrite the existing \"extract_\" dir, if False, will throw and error if a dir \"extract_*\" is detected in the .D dir.\n",
    "    \"\"\"\n",
    "    metadata, data = get_data(path=path)\n",
    "\n",
    "    dir_pattern = \"extract_\"\n",
    "\n",
    "    old_dir_glob = list(path.glob(f\"{dir_pattern}*\"))\n",
    "\n",
    "    old_dirpath = old_dir_glob[0]\n",
    "\n",
    "    if not old_dirpath:\n",
    "        return write_extraction_dir(\n",
    "            extract_path_prefix=path / dir_pattern, metadata=metadata, data=data\n",
    "        )\n",
    "    elif len(old_dir_glob) > 1:\n",
    "        raise ValueError(\n",
    "            \"multiple dirs with 'extract_' pattern detected. Please remove\"\n",
    "        )\n",
    "    elif old_dir_glob and overwrite:\n",
    "        shutil.rmtree(old_dirpath, ignore_errors=False)\n",
    "        return write_extraction_dir(\n",
    "            extract_path_prefix=path / dir_pattern, metadata=metadata, data=data\n",
    "        )\n",
    "    elif old_dirpath and not overwrite:\n",
    "        raise ValueError(\n",
    "            \"old extraction dir detected, set overwrite = True to overwrite\"\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"unexpected logic path encountered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_run_data(Path(d_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it for all dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_path = \"/Users/jonathan/mres_thesis/database_etl/data/raw_uv\"\n",
    "\n",
    "d_paths = list(Path(lib_path).glob(\"*.D\"))\n",
    "d_paths[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will copy the above definitions to `code.chm_extractor`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database-etl-SYkgHnDq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
