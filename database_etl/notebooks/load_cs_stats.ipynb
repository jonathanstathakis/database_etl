{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "cdt: 2024-09-19T21:55:13\n",
    "title: Load CS Image File Statimgtics\n",
    "description: creation of a CS stats table - dimensions, hertz, maxima, etc.\n",
    "project: database_etl\n",
    "conclusion:\n",
    "status: open\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "\n",
    "%xmode minimal\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from database_etl.definitions import DB_PATH, RAW_DATA_LIB\n",
    "\n",
    "pl.Config.set_tbl_rows(99).set_fmt_str_lengths(999)\n",
    "con = db.connect(DB_PATH)\n",
    "\n",
    "overwrite: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to iterate through all the .D dirs and measure statistics from all the files. First we need to get the data parquet of each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list(RAW_DATA_LIB.glob(\"*.D/extract_*/data.parquet\"))\n",
    "len(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Now iterate over them. Do we use duckdb or polars for EDA? lets test it. Iterate over, calculate the cumsum of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_cumsums = {}\n",
    "\n",
    "for f in image_files:\n",
    "    key = f.parts[-3]\n",
    "    pl_cumsums[key] = pl.read_parquet(f).select(pl.exclude(\"id\").sum())\n",
    "list(pl_cumsums.values())[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cumsums = {}\n",
    "\n",
    "for idx, f in enumerate(image_files):\n",
    "    key = f.parts[-3]\n",
    "    result = con.sql(f\"select sum(columns(* exclude id)) from read_parquet('{str(f)}')\")\n",
    "    pl_cumsums[key] = result\n",
    "len(pl_cumsums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "For that opeation, duckdb wins at 0.1 second. However it really isnt designed for that style of data, and is a paaaaaain to work with. Stick with polars to calculate the aggregates then duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to obtain the time ranges, hertz, wavelength ranges of all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good, now to load into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stats_from_files(image_files: list[Path]) -> pl.DataFrame:\n",
    "    image_stats_ = []\n",
    "\n",
    "    for f in list(image_files):\n",
    "        df = pl.read_parquet(f)\n",
    "\n",
    "        # nm\n",
    "        nm_min = df.columns[2]\n",
    "        nm_max = df.columns[-1]\n",
    "        nm_count = len(df.columns)\n",
    "\n",
    "        # time\n",
    "        mins_min = df.select(\"time\").min().item()\n",
    "        mins_max = df.select(\"time\").max().item()\n",
    "        mins_count = df.select(\"time\").count().item()\n",
    "\n",
    "        # abs @ 256\n",
    "        abs_min = df.select(\"256\").min().item()\n",
    "        abs_max = df.select(\"256\").max().item()\n",
    "        abs_argmin = df.select(\"256\").to_series().arg_min()\n",
    "        abs_argmax = df.select(\"256\").to_series().arg_max()\n",
    "\n",
    "        # hertz\n",
    "        hertz = df.select(pl.col(\"time\").diff().mul(60).pow(-1).mean().round(2))\n",
    "\n",
    "        image_stats_.append(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"id\": df.select(\"id\")[0].item(),\n",
    "                    \"nm_min\": nm_min,\n",
    "                    \"nm_max\": nm_max,\n",
    "                    \"nm_count\": nm_count,\n",
    "                    \"mins_min\": mins_min,\n",
    "                    \"mins_max\": mins_max,\n",
    "                    \"mins_count\": mins_count,\n",
    "                    \"abs_min\": abs_min,\n",
    "                    \"abs_max\": abs_max,\n",
    "                    \"abs_argmin\": abs_argmin,\n",
    "                    \"abs_argmax\": abs_argmax,\n",
    "                    \"hertz\": hertz,\n",
    "                    \"path\": str(f),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    image_stats_df = pl.concat(image_stats_)\n",
    "\n",
    "    return image_stats_df\n",
    "\n",
    "\n",
    "def load_image_stats_to_db(con: db.DuckDBPyConnection, image_stats_df: pl.DataFrame):\n",
    "    image_stats_df = image_stats_df  # to fool lsp\n",
    "    con.sql(\n",
    "        \"\"\"--sql\n",
    "    create or replace table image_stats (\n",
    "        pk integer primary key,\n",
    "        nm_min integer not null,\n",
    "        nm_max integer not null,\n",
    "        nm_count integer not null,\n",
    "        mins_min float not null,\n",
    "        mins_max float not null,\n",
    "        mins_count integer not null,\n",
    "        abs_min float not null,\n",
    "        abs_max float not null,\n",
    "        abs_argmin float not null,\n",
    "        abs_argmax float not null,\n",
    "        hertz float not null,\n",
    "        path varchar not null unique,\n",
    "    );\n",
    "    insert into image_stats\n",
    "        select\n",
    "            chm.pk,\n",
    "            img.nm_min,\n",
    "            img.nm_max,\n",
    "            img.nm_count,\n",
    "            img.mins_min,\n",
    "            img.mins_max,\n",
    "            img.mins_count as mins_count,\n",
    "            img.abs_min,\n",
    "            img.abs_max,\n",
    "            img.abs_argmin,\n",
    "            img.abs_argmax,\n",
    "            img.hertz,\n",
    "            img.path\n",
    "        from\n",
    "            image_stats_df img\n",
    "        join\n",
    "            chm\n",
    "        using\n",
    "            (id)\n",
    "        order by\n",
    "            chm.pk;\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_image_stats(image_files: list[Path], con: db.DuckDBPyConnection) -> None:\n",
    "    image_stats_df = image_stats_from_files(image_files=image_files)\n",
    "    load_image_stats_to_db(con=con, image_stats_df=image_stats_df)\n",
    "\n",
    "\n",
    "if overwrite:\n",
    "    load_image_stats(image_files=image_files, con=con)\n",
    "    con.sql(\n",
    "        \"\"\"--sql\n",
    "    select * from image_stats limit 5\n",
    "    \"\"\"\n",
    "    ).pl().pipe(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great. Now find the outliers for each,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\n",
    "    \"\"\"--sql\n",
    "select\n",
    "    count( distinct COLUMNS(* exclude pk))\n",
    "from\n",
    "    image_stats\n",
    "\"\"\"\n",
    ").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the wavelengths are all the same, but there are 6 distinct mins maximums, and 2 distinct hertz.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\n",
    "    \"\"\"--sql\n",
    "select\n",
    "    distinct(hertz)\n",
    "from\n",
    "    image_stats\n",
    "\"\"\"\n",
    ").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()\n",
    "del con"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database-etl-SYkgHnDq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
